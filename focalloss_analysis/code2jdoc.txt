06/12/2020 03:47:44 PM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 32 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/12/2020 03:47:44 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:44 PM: [ Load and process data files ]
06/12/2020 03:47:50 PM: [ Num train examples = 69708 ]
06/12/2020 03:47:50 PM: [ Dataset weights = {0: 1.0} ]
06/12/2020 03:47:51 PM: [ Num dev examples = 8714 ]
06/12/2020 03:47:51 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:51 PM: [ Training model from scratch... ]
06/12/2020 03:47:51 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:51 PM: [ Build word dictionary ]
06/12/2020 03:47:55 PM: [ Num words in source = 34131 and target = 28239 ]
06/12/2020 03:47:55 PM: [ Trainable #parameters [encoder-decoder] 44.2M [total] 77M ]
06/12/2020 03:47:55 PM: [ Breakdown of the trainable paramters
+------------------------------------------------------------------------------+--------------+----------+
| Layer Name                                                                   | Output Shape |  Param # |
+------------------------------------------------------------------------------+--------------+----------+
| embedder.src_word_embeddings.make_embedding.emb_luts.0.weight                | [34131, 512] | 17475072 |
| embedder.tgt_word_embeddings.make_embedding.emb_luts.0.weight                | [28239, 512] | 14458368 |
| embedder.tgt_pos_embeddings.weight                                           |    [52, 512] |    26624 |
| encoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| generator.bias                                                               |      [28239] |    28239 |
| copy_attn.linear_in.weight                                                   |   [512, 512] |   262144 |
| copy_attn.linear_out.weight                                                  |  [512, 1024] |   524288 |
| copy_generator.linear_copy.weight                                            |     [1, 512] |      512 |
| copy_generator.linear_copy.bias                                              |          [1] |        1 |
+------------------------------------------------------------------------------+--------------+----------+ ]
06/12/2020 03:47:58 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:58 PM: [ Make data loaders ]
06/12/2020 03:47:58 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:58 PM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 32,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/12/2020 03:47:58 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:47:58 PM: [ Starting training... ]
06/12/2020 03:49:03 PM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/12/2020 03:49:03 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:03 PM: [ Load and process data files ]
06/12/2020 03:49:08 PM: [ Num train examples = 69708 ]
06/12/2020 03:49:08 PM: [ Dataset weights = {0: 1.0} ]
06/12/2020 03:49:09 PM: [ Num dev examples = 8714 ]
06/12/2020 03:49:09 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:09 PM: [ Training model from scratch... ]
06/12/2020 03:49:09 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:09 PM: [ Build word dictionary ]
06/12/2020 03:49:13 PM: [ Num words in source = 34131 and target = 28239 ]
06/12/2020 03:49:14 PM: [ Trainable #parameters [encoder-decoder] 44.2M [total] 77M ]
06/12/2020 03:49:14 PM: [ Breakdown of the trainable paramters
+------------------------------------------------------------------------------+--------------+----------+
| Layer Name                                                                   | Output Shape |  Param # |
+------------------------------------------------------------------------------+--------------+----------+
| embedder.src_word_embeddings.make_embedding.emb_luts.0.weight                | [34131, 512] | 17475072 |
| embedder.tgt_word_embeddings.make_embedding.emb_luts.0.weight                | [28239, 512] | 14458368 |
| embedder.tgt_pos_embeddings.weight                                           |    [52, 512] |    26624 |
| encoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| encoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| encoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| encoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| encoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_k.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.attention.relative_positions_embeddings_v.weight |     [65, 64] |     4160 |
| encoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| encoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| encoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| encoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| encoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| encoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.0.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.0.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.0.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.0.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.0.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.0.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.0.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.0.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.0.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.0.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.1.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.1.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.1.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.1.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.1.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.1.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.1.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.1.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.1.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.1.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.2.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.2.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.2.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.2.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.2.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.2.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.2.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.2.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.2.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.2.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.3.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.3.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.3.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.3.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.3.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.3.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.3.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.3.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.3.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.3.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.4.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.4.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.4.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.4.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.4.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.4.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.4.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.4.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.4.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.4.feed_forward.layer_norm.bias                     |        [512] |      512 |
| decoder.transformer.layer.5.attention.key.weight                             |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.key.bias                               |        [512] |      512 |
| decoder.transformer.layer.5.attention.query.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.query.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.value.weight                           |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.value.bias                             |        [512] |      512 |
| decoder.transformer.layer.5.attention.output.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.attention.output.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.weight                                |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm.bias                                  |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.key.weight                          |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.key.bias                            |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.query.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.query.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.value.weight                        |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.value.bias                          |        [512] |      512 |
| decoder.transformer.layer.5.context_attn.output.weight                       |   [512, 512] |   262144 |
| decoder.transformer.layer.5.context_attn.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.weight                              |        [512] |      512 |
| decoder.transformer.layer.5.layer_norm_2.bias                                |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.intermediate.weight                 |  [2048, 512] |  1048576 |
| decoder.transformer.layer.5.feed_forward.intermediate.bias                   |       [2048] |     2048 |
| decoder.transformer.layer.5.feed_forward.output.weight                       |  [512, 2048] |  1048576 |
| decoder.transformer.layer.5.feed_forward.output.bias                         |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.weight                   |        [512] |      512 |
| decoder.transformer.layer.5.feed_forward.layer_norm.bias                     |        [512] |      512 |
| generator.bias                                                               |      [28239] |    28239 |
| copy_attn.linear_in.weight                                                   |   [512, 512] |   262144 |
| copy_attn.linear_out.weight                                                  |  [512, 1024] |   524288 |
| copy_generator.linear_copy.weight                                            |     [1, 512] |      512 |
| copy_generator.linear_copy.bias                                              |          [1] |        1 |
+------------------------------------------------------------------------------+--------------+----------+ ]
06/12/2020 03:49:16 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:16 PM: [ Make data loaders ]
06/12/2020 03:49:16 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:17 PM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/12/2020 03:49:17 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/12/2020 03:49:17 PM: [ Starting training... ]
06/12/2020 03:56:03 PM: [ train: Epoch 1 | perplexity = 22026.47 | ml_loss = 452.92 | Time for epoch = 406.76 (s) ]
06/12/2020 03:58:45 PM: [ dev valid official: Epoch = 1 | bleu = 3.31 | rouge_l = 5.53 | Precision = 4.84 | Recall = 11.51 | F1 = 5.52 | examples = 8714 | valid time = 158.37 (s) ]
06/12/2020 03:58:45 PM: [ Best valid: bleu = 3.31 (epoch 1, 727 updates) ]
06/12/2020 04:05:29 PM: [ train: Epoch 2 | perplexity = 21988.71 | ml_loss = 273.44 | Time for epoch = 402.48 (s) ]
06/12/2020 04:08:06 PM: [ dev valid official: Epoch = 2 | bleu = 8.71 | rouge_l = 17.22 | Precision = 21.51 | Recall = 20.78 | F1 = 18.28 | examples = 8714 | valid time = 154.07 (s) ]
06/12/2020 04:08:06 PM: [ Best valid: bleu = 8.71 (epoch 2, 1454 updates) ]
06/12/2020 04:14:54 PM: [ train: Epoch 3 | perplexity = 20825.26 | ml_loss = 210.97 | Time for epoch = 406.59 (s) ]
06/12/2020 04:17:30 PM: [ dev valid official: Epoch = 3 | bleu = 9.13 | rouge_l = 15.16 | Precision = 21.98 | Recall = 17.55 | F1 = 16.70 | examples = 8714 | valid time = 152.05 (s) ]
06/12/2020 04:17:30 PM: [ Best valid: bleu = 9.13 (epoch 3, 2181 updates) ]
06/12/2020 04:24:21 PM: [ train: Epoch 4 | perplexity = 14481.67 | ml_loss = 178.61 | Time for epoch = 410.22 (s) ]
06/12/2020 04:26:56 PM: [ dev valid official: Epoch = 4 | bleu = 9.65 | rouge_l = 14.57 | Precision = 28.87 | Recall = 13.30 | F1 = 16.19 | examples = 8714 | valid time = 151.00 (s) ]
06/12/2020 04:26:56 PM: [ Best valid: bleu = 9.65 (epoch 4, 2908 updates) ]
06/12/2020 04:34:41 PM: [ train: Epoch 5 | perplexity = 5130.27 | ml_loss = 151.12 | Time for epoch = 463.97 (s) ]
06/12/2020 04:37:15 PM: [ dev valid official: Epoch = 5 | bleu = 9.67 | rouge_l = 17.61 | Precision = 38.70 | Recall = 15.54 | F1 = 19.81 | examples = 8714 | valid time = 150.29 (s) ]
06/12/2020 04:37:15 PM: [ Best valid: bleu = 9.67 (epoch 5, 3635 updates) ]
06/12/2020 04:45:07 PM: [ train: Epoch 6 | perplexity = 1884.48 | ml_loss = 133.53 | Time for epoch = 471.15 (s) ]
06/12/2020 04:47:44 PM: [ dev valid official: Epoch = 6 | bleu = 11.80 | rouge_l = 23.78 | Precision = 35.26 | Recall = 24.93 | F1 = 25.82 | examples = 8714 | valid time = 152.60 (s) ]
06/12/2020 04:47:44 PM: [ Best valid: bleu = 11.80 (epoch 6, 4362 updates) ]
06/12/2020 04:54:50 PM: [ train: Epoch 7 | perplexity = 677.46 | ml_loss = 114.78 | Time for epoch = 425.46 (s) ]
06/12/2020 04:57:27 PM: [ dev valid official: Epoch = 7 | bleu = 12.19 | rouge_l = 26.02 | Precision = 38.18 | Recall = 26.74 | F1 = 27.75 | examples = 8714 | valid time = 153.47 (s) ]
06/12/2020 04:57:27 PM: [ Best valid: bleu = 12.19 (epoch 7, 5089 updates) ]
06/12/2020 05:05:00 PM: [ train: Epoch 8 | perplexity = 268.84 | ml_loss = 99.55 | Time for epoch = 450.77 (s) ]
06/12/2020 05:07:36 PM: [ dev valid official: Epoch = 8 | bleu = 13.25 | rouge_l = 26.88 | Precision = 38.82 | Recall = 27.93 | F1 = 28.89 | examples = 8714 | valid time = 152.93 (s) ]
06/12/2020 05:07:36 PM: [ Best valid: bleu = 13.25 (epoch 8, 5816 updates) ]
06/12/2020 05:15:09 PM: [ train: Epoch 9 | perplexity = 147.17 | ml_loss = 89.30 | Time for epoch = 451.70 (s) ]
06/12/2020 05:17:44 PM: [ dev valid official: Epoch = 9 | bleu = 14.02 | rouge_l = 28.66 | Precision = 40.59 | Recall = 29.68 | F1 = 30.69 | examples = 8714 | valid time = 151.85 (s) ]
06/12/2020 05:17:44 PM: [ Best valid: bleu = 14.02 (epoch 9, 6543 updates) ]
06/12/2020 05:24:37 PM: [ train: Epoch 10 | perplexity = 99.18 | ml_loss = 82.33 | Time for epoch = 411.75 (s) ]
06/12/2020 05:27:16 PM: [ dev valid official: Epoch = 10 | bleu = 12.60 | rouge_l = 25.62 | Precision = 36.66 | Recall = 27.24 | F1 = 27.24 | examples = 8714 | valid time = 155.40 (s) ]
06/12/2020 05:35:01 PM: [ train: Epoch 11 | perplexity = 73.64 | ml_loss = 77.51 | Time for epoch = 464.51 (s) ]
06/12/2020 05:37:38 PM: [ dev valid official: Epoch = 11 | bleu = 15.11 | rouge_l = 29.97 | Precision = 44.59 | Recall = 29.65 | F1 = 32.22 | examples = 8714 | valid time = 153.12 (s) ]
06/12/2020 05:37:38 PM: [ Best valid: bleu = 15.11 (epoch 11, 7997 updates) ]
06/12/2020 05:44:38 PM: [ train: Epoch 12 | perplexity = 62.09 | ml_loss = 74.26 | Time for epoch = 418.64 (s) ]
06/12/2020 05:47:14 PM: [ dev valid official: Epoch = 12 | bleu = 16.10 | rouge_l = 31.32 | Precision = 43.92 | Recall = 32.06 | F1 = 33.48 | examples = 8714 | valid time = 153.09 (s) ]
06/12/2020 05:47:14 PM: [ Best valid: bleu = 16.10 (epoch 12, 8724 updates) ]
06/12/2020 05:54:42 PM: [ train: Epoch 13 | perplexity = 53.36 | ml_loss = 71.70 | Time for epoch = 446.21 (s) ]
06/12/2020 05:57:19 PM: [ dev valid official: Epoch = 13 | bleu = 17.11 | rouge_l = 32.27 | Precision = 47.00 | Recall = 32.07 | F1 = 34.69 | examples = 8714 | valid time = 154.07 (s) ]
06/12/2020 05:57:19 PM: [ Best valid: bleu = 17.11 (epoch 13, 9451 updates) ]
06/12/2020 06:04:37 PM: [ train: Epoch 14 | perplexity = 47.42 | ml_loss = 69.58 | Time for epoch = 435.84 (s) ]
06/12/2020 06:07:15 PM: [ dev valid official: Epoch = 14 | bleu = 17.75 | rouge_l = 33.46 | Precision = 46.13 | Recall = 34.30 | F1 = 35.73 | examples = 8714 | valid time = 154.46 (s) ]
06/12/2020 06:07:15 PM: [ Best valid: bleu = 17.75 (epoch 14, 10178 updates) ]
06/12/2020 06:15:00 PM: [ train: Epoch 15 | perplexity = 41.80 | ml_loss = 67.68 | Time for epoch = 463.85 (s) ]
06/12/2020 06:17:38 PM: [ dev valid official: Epoch = 15 | bleu = 18.58 | rouge_l = 33.05 | Precision = 46.21 | Recall = 33.31 | F1 = 35.39 | examples = 8714 | valid time = 154.22 (s) ]
06/12/2020 06:17:38 PM: [ Best valid: bleu = 18.58 (epoch 15, 10905 updates) ]
06/12/2020 06:24:45 PM: [ train: Epoch 16 | perplexity = 38.36 | ml_loss = 65.82 | Time for epoch = 426.32 (s) ]
06/12/2020 06:27:24 PM: [ dev valid official: Epoch = 16 | bleu = 18.85 | rouge_l = 34.12 | Precision = 45.77 | Recall = 35.32 | F1 = 36.30 | examples = 8714 | valid time = 154.78 (s) ]
06/12/2020 06:27:24 PM: [ Best valid: bleu = 18.85 (epoch 16, 11632 updates) ]
06/12/2020 06:34:19 PM: [ train: Epoch 17 | perplexity = 35.33 | ml_loss = 64.24 | Time for epoch = 413.92 (s) ]
06/12/2020 06:36:55 PM: [ dev valid official: Epoch = 17 | bleu = 20.24 | rouge_l = 35.54 | Precision = 49.94 | Recall = 35.39 | F1 = 38.01 | examples = 8714 | valid time = 153.20 (s) ]
06/12/2020 06:36:55 PM: [ Best valid: bleu = 20.24 (epoch 17, 12359 updates) ]
06/12/2020 06:43:52 PM: [ train: Epoch 18 | perplexity = 32.35 | ml_loss = 62.74 | Time for epoch = 415.56 (s) ]
06/12/2020 06:46:29 PM: [ dev valid official: Epoch = 18 | bleu = 20.86 | rouge_l = 36.68 | Precision = 50.56 | Recall = 36.95 | F1 = 39.09 | examples = 8714 | valid time = 153.04 (s) ]
06/12/2020 06:46:29 PM: [ Best valid: bleu = 20.86 (epoch 18, 13086 updates) ]
06/12/2020 06:53:47 PM: [ train: Epoch 19 | perplexity = 29.41 | ml_loss = 61.30 | Time for epoch = 436.23 (s) ]
06/12/2020 06:56:24 PM: [ dev valid official: Epoch = 19 | bleu = 21.47 | rouge_l = 37.15 | Precision = 48.31 | Recall = 38.81 | F1 = 39.51 | examples = 8714 | valid time = 154.08 (s) ]
06/12/2020 06:56:24 PM: [ Best valid: bleu = 21.47 (epoch 19, 13813 updates) ]
06/12/2020 07:03:55 PM: [ train: Epoch 20 | perplexity = 27.01 | ml_loss = 59.86 | Time for epoch = 449.60 (s) ]
06/12/2020 07:06:33 PM: [ dev valid official: Epoch = 20 | bleu = 22.01 | rouge_l = 37.55 | Precision = 51.83 | Recall = 37.61 | F1 = 40.05 | examples = 8714 | valid time = 153.89 (s) ]
06/12/2020 07:06:33 PM: [ Best valid: bleu = 22.01 (epoch 20, 14540 updates) ]
06/12/2020 07:13:24 PM: [ train: Epoch 21 | perplexity = 25.63 | ml_loss = 58.62 | Time for epoch = 409.88 (s) ]
06/12/2020 07:16:02 PM: [ dev valid official: Epoch = 21 | bleu = 22.19 | rouge_l = 38.33 | Precision = 49.82 | Recall = 40.07 | F1 = 40.71 | examples = 8714 | valid time = 153.75 (s) ]
06/12/2020 07:16:02 PM: [ Best valid: bleu = 22.19 (epoch 21, 15267 updates) ]
06/12/2020 07:23:54 PM: [ train: Epoch 22 | perplexity = 23.08 | ml_loss = 57.44 | Time for epoch = 471.19 (s) ]
06/12/2020 07:26:33 PM: [ dev valid official: Epoch = 22 | bleu = 23.32 | rouge_l = 38.80 | Precision = 52.15 | Recall = 39.29 | F1 = 41.30 | examples = 8714 | valid time = 154.22 (s) ]
06/12/2020 07:26:33 PM: [ Best valid: bleu = 23.32 (epoch 22, 15994 updates) ]
06/12/2020 07:33:36 PM: [ train: Epoch 23 | perplexity = 22.10 | ml_loss = 56.17 | Time for epoch = 422.45 (s) ]
06/12/2020 07:36:13 PM: [ dev valid official: Epoch = 23 | bleu = 23.30 | rouge_l = 38.36 | Precision = 49.60 | Recall = 39.90 | F1 = 40.65 | examples = 8714 | valid time = 153.49 (s) ]
06/12/2020 07:42:59 PM: [ train: Epoch 24 | perplexity = 20.85 | ml_loss = 55.02 | Time for epoch = 405.46 (s) ]
06/12/2020 07:45:36 PM: [ dev valid official: Epoch = 24 | bleu = 24.10 | rouge_l = 39.46 | Precision = 51.18 | Recall = 40.55 | F1 = 41.83 | examples = 8714 | valid time = 153.85 (s) ]
06/12/2020 07:45:36 PM: [ Best valid: bleu = 24.10 (epoch 24, 17448 updates) ]
06/12/2020 07:52:53 PM: [ train: Epoch 25 | perplexity = 19.22 | ml_loss = 53.94 | Time for epoch = 435.85 (s) ]
06/12/2020 07:55:30 PM: [ dev valid official: Epoch = 25 | bleu = 24.30 | rouge_l = 39.82 | Precision = 50.01 | Recall = 41.88 | F1 = 42.03 | examples = 8714 | valid time = 152.72 (s) ]
06/12/2020 07:55:30 PM: [ Best valid: bleu = 24.30 (epoch 25, 18175 updates) ]
06/12/2020 08:02:14 PM: [ train: Epoch 26 | perplexity = 18.50 | ml_loss = 52.93 | Time for epoch = 403.62 (s) ]
06/12/2020 08:04:51 PM: [ dev valid official: Epoch = 26 | bleu = 24.72 | rouge_l = 39.90 | Precision = 50.29 | Recall = 41.74 | F1 = 42.15 | examples = 8714 | valid time = 152.55 (s) ]
06/12/2020 08:04:51 PM: [ Best valid: bleu = 24.72 (epoch 26, 18902 updates) ]
06/12/2020 08:11:56 PM: [ train: Epoch 27 | perplexity = 17.20 | ml_loss = 51.84 | Time for epoch = 423.67 (s) ]
06/12/2020 08:14:33 PM: [ dev valid official: Epoch = 27 | bleu = 26.02 | rouge_l = 41.08 | Precision = 53.46 | Recall = 41.76 | F1 = 43.45 | examples = 8714 | valid time = 153.35 (s) ]
06/12/2020 08:14:33 PM: [ Best valid: bleu = 26.02 (epoch 27, 19629 updates) ]
06/12/2020 08:22:17 PM: [ train: Epoch 28 | perplexity = 15.81 | ml_loss = 50.85 | Time for epoch = 462.38 (s) ]
06/12/2020 08:24:54 PM: [ dev valid official: Epoch = 28 | bleu = 26.17 | rouge_l = 41.26 | Precision = 53.60 | Recall = 41.95 | F1 = 43.65 | examples = 8714 | valid time = 153.71 (s) ]
06/12/2020 08:24:54 PM: [ Best valid: bleu = 26.17 (epoch 28, 20356 updates) ]
06/12/2020 08:31:41 PM: [ train: Epoch 29 | perplexity = 15.56 | ml_loss = 49.90 | Time for epoch = 405.43 (s) ]
06/12/2020 08:34:17 PM: [ dev valid official: Epoch = 29 | bleu = 26.04 | rouge_l = 41.53 | Precision = 50.53 | Recall = 44.61 | F1 = 43.81 | examples = 8714 | valid time = 152.35 (s) ]
06/12/2020 08:41:55 PM: [ train: Epoch 30 | perplexity = 14.21 | ml_loss = 48.99 | Time for epoch = 457.95 (s) ]
06/12/2020 08:44:31 PM: [ dev valid official: Epoch = 30 | bleu = 27.00 | rouge_l = 42.40 | Precision = 53.64 | Recall = 43.82 | F1 = 44.73 | examples = 8714 | valid time = 152.60 (s) ]
06/12/2020 08:44:31 PM: [ Best valid: bleu = 27.00 (epoch 30, 21810 updates) ]
06/12/2020 08:51:41 PM: [ train: Epoch 31 | perplexity = 13.71 | ml_loss = 48.07 | Time for epoch = 428.62 (s) ]
06/12/2020 08:54:17 PM: [ dev valid official: Epoch = 31 | bleu = 27.55 | rouge_l = 42.79 | Precision = 54.78 | Recall = 43.54 | F1 = 45.13 | examples = 8714 | valid time = 152.56 (s) ]
06/12/2020 08:54:17 PM: [ Best valid: bleu = 27.55 (epoch 31, 22537 updates) ]
06/12/2020 09:01:04 PM: [ train: Epoch 32 | perplexity = 13.30 | ml_loss = 47.25 | Time for epoch = 406.05 (s) ]
06/12/2020 09:03:41 PM: [ dev valid official: Epoch = 32 | bleu = 27.48 | rouge_l = 42.04 | Precision = 55.05 | Recall = 42.24 | F1 = 44.40 | examples = 8714 | valid time = 152.28 (s) ]
06/12/2020 09:10:47 PM: [ train: Epoch 33 | perplexity = 12.53 | ml_loss = 46.37 | Time for epoch = 426.55 (s) ]
06/12/2020 09:13:23 PM: [ dev valid official: Epoch = 33 | bleu = 27.97 | rouge_l = 42.74 | Precision = 52.68 | Recall = 44.66 | F1 = 44.93 | examples = 8714 | valid time = 152.47 (s) ]
06/12/2020 09:13:23 PM: [ Best valid: bleu = 27.97 (epoch 33, 23991 updates) ]
06/12/2020 09:20:19 PM: [ train: Epoch 34 | perplexity = 11.96 | ml_loss = 45.55 | Time for epoch = 414.90 (s) ]
06/12/2020 09:22:57 PM: [ dev valid official: Epoch = 34 | bleu = 28.32 | rouge_l = 42.74 | Precision = 54.85 | Recall = 43.30 | F1 = 45.11 | examples = 8714 | valid time = 153.53 (s) ]
06/12/2020 09:22:57 PM: [ Best valid: bleu = 28.32 (epoch 34, 24718 updates) ]
06/12/2020 09:30:11 PM: [ train: Epoch 35 | perplexity = 11.39 | ml_loss = 44.80 | Time for epoch = 433.10 (s) ]
06/12/2020 09:32:48 PM: [ dev valid official: Epoch = 35 | bleu = 28.84 | rouge_l = 43.62 | Precision = 54.78 | Recall = 44.66 | F1 = 45.88 | examples = 8714 | valid time = 152.83 (s) ]
06/12/2020 09:32:48 PM: [ Best valid: bleu = 28.84 (epoch 35, 25445 updates) ]
06/12/2020 09:40:28 PM: [ train: Epoch 36 | perplexity = 10.66 | ml_loss = 44.00 | Time for epoch = 458.80 (s) ]
06/12/2020 09:43:05 PM: [ dev valid official: Epoch = 36 | bleu = 29.18 | rouge_l = 43.71 | Precision = 55.00 | Recall = 44.81 | F1 = 45.98 | examples = 8714 | valid time = 153.27 (s) ]
06/12/2020 09:43:05 PM: [ Best valid: bleu = 29.18 (epoch 36, 26172 updates) ]
06/12/2020 09:50:19 PM: [ train: Epoch 37 | perplexity = 10.39 | ml_loss = 43.29 | Time for epoch = 432.95 (s) ]
06/12/2020 09:52:55 PM: [ dev valid official: Epoch = 37 | bleu = 29.50 | rouge_l = 44.00 | Precision = 55.01 | Recall = 45.12 | F1 = 46.29 | examples = 8714 | valid time = 152.50 (s) ]
06/12/2020 09:52:55 PM: [ Best valid: bleu = 29.50 (epoch 37, 26899 updates) ]
06/12/2020 10:00:13 PM: [ train: Epoch 38 | perplexity = 9.92 | ml_loss = 42.52 | Time for epoch = 436.03 (s) ]
06/12/2020 10:02:50 PM: [ dev valid official: Epoch = 38 | bleu = 29.68 | rouge_l = 43.89 | Precision = 54.82 | Recall = 45.10 | F1 = 46.16 | examples = 8714 | valid time = 153.80 (s) ]
06/12/2020 10:02:50 PM: [ Best valid: bleu = 29.68 (epoch 38, 27626 updates) ]
06/12/2020 10:10:36 PM: [ train: Epoch 39 | perplexity = 9.34 | ml_loss = 41.81 | Time for epoch = 464.24 (s) ]
06/12/2020 10:13:12 PM: [ dev valid official: Epoch = 39 | bleu = 29.97 | rouge_l = 43.90 | Precision = 56.72 | Recall = 43.91 | F1 = 46.18 | examples = 8714 | valid time = 153.29 (s) ]
06/12/2020 10:13:12 PM: [ Best valid: bleu = 29.97 (epoch 39, 28353 updates) ]
06/12/2020 10:20:32 PM: [ train: Epoch 40 | perplexity = 9.18 | ml_loss = 41.11 | Time for epoch = 438.86 (s) ]
06/12/2020 10:23:11 PM: [ dev valid official: Epoch = 40 | bleu = 30.21 | rouge_l = 43.90 | Precision = 56.23 | Recall = 44.24 | F1 = 46.27 | examples = 8714 | valid time = 154.98 (s) ]
06/12/2020 10:23:11 PM: [ Best valid: bleu = 30.21 (epoch 40, 29080 updates) ]
06/12/2020 10:30:09 PM: [ train: Epoch 41 | perplexity = 8.96 | ml_loss = 40.45 | Time for epoch = 416.61 (s) ]
06/12/2020 10:32:46 PM: [ dev valid official: Epoch = 41 | bleu = 30.60 | rouge_l = 44.31 | Precision = 55.96 | Recall = 44.72 | F1 = 46.57 | examples = 8714 | valid time = 153.83 (s) ]
06/12/2020 10:32:46 PM: [ Best valid: bleu = 30.60 (epoch 41, 29807 updates) ]
06/12/2020 10:40:35 PM: [ train: Epoch 42 | perplexity = 8.31 | ml_loss = 39.79 | Time for epoch = 466.98 (s) ]
06/12/2020 10:43:12 PM: [ dev valid official: Epoch = 42 | bleu = 30.71 | rouge_l = 44.72 | Precision = 56.49 | Recall = 45.18 | F1 = 47.00 | examples = 8714 | valid time = 153.43 (s) ]
06/12/2020 10:43:12 PM: [ Best valid: bleu = 30.71 (epoch 42, 30534 updates) ]
06/12/2020 10:50:59 PM: [ train: Epoch 43 | perplexity = 7.96 | ml_loss = 39.10 | Time for epoch = 465.32 (s) ]
06/12/2020 10:53:36 PM: [ dev valid official: Epoch = 43 | bleu = 31.21 | rouge_l = 45.15 | Precision = 56.02 | Recall = 46.13 | F1 = 47.36 | examples = 8714 | valid time = 153.81 (s) ]
06/12/2020 10:53:36 PM: [ Best valid: bleu = 31.21 (epoch 43, 31261 updates) ]
06/12/2020 11:01:06 PM: [ train: Epoch 44 | perplexity = 7.80 | ml_loss = 38.51 | Time for epoch = 448.12 (s) ]
06/12/2020 11:03:41 PM: [ dev valid official: Epoch = 44 | bleu = 31.37 | rouge_l = 45.53 | Precision = 55.46 | Recall = 47.15 | F1 = 47.73 | examples = 8714 | valid time = 152.15 (s) ]
06/12/2020 11:03:41 PM: [ Best valid: bleu = 31.37 (epoch 44, 31988 updates) ]
06/12/2020 11:11:15 PM: [ train: Epoch 45 | perplexity = 7.46 | ml_loss = 37.80 | Time for epoch = 452.58 (s) ]
06/12/2020 11:13:52 PM: [ dev valid official: Epoch = 45 | bleu = 31.70 | rouge_l = 45.49 | Precision = 56.78 | Recall = 46.16 | F1 = 47.72 | examples = 8714 | valid time = 153.37 (s) ]
06/12/2020 11:13:52 PM: [ Best valid: bleu = 31.70 (epoch 45, 32715 updates) ]
06/12/2020 11:21:33 PM: [ train: Epoch 46 | perplexity = 7.20 | ml_loss = 37.27 | Time for epoch = 459.72 (s) ]
06/12/2020 11:24:10 PM: [ dev valid official: Epoch = 46 | bleu = 32.25 | rouge_l = 45.98 | Precision = 57.36 | Recall = 46.48 | F1 = 48.16 | examples = 8714 | valid time = 153.38 (s) ]
06/12/2020 11:24:10 PM: [ Best valid: bleu = 32.25 (epoch 46, 33442 updates) ]
06/12/2020 11:31:15 PM: [ train: Epoch 47 | perplexity = 7.10 | ml_loss = 36.62 | Time for epoch = 423.88 (s) ]
06/12/2020 11:33:50 PM: [ dev valid official: Epoch = 47 | bleu = 32.29 | rouge_l = 45.76 | Precision = 56.98 | Recall = 46.41 | F1 = 48.04 | examples = 8714 | valid time = 151.26 (s) ]
06/12/2020 11:33:50 PM: [ Best valid: bleu = 32.29 (epoch 47, 34169 updates) ]
06/12/2020 11:41:05 PM: [ train: Epoch 48 | perplexity = 6.85 | ml_loss = 36.11 | Time for epoch = 433.30 (s) ]
06/12/2020 11:43:39 PM: [ dev valid official: Epoch = 48 | bleu = 32.67 | rouge_l = 46.26 | Precision = 56.28 | Recall = 47.42 | F1 = 48.42 | examples = 8714 | valid time = 151.13 (s) ]
06/12/2020 11:43:39 PM: [ Best valid: bleu = 32.67 (epoch 48, 34896 updates) ]
06/13/2020 04:44:27 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/13/2020 04:44:27 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 04:44:27 AM: [ Load and process data files ]
06/13/2020 04:44:37 AM: [ Num train examples = 69708 ]
06/13/2020 04:44:37 AM: [ Dataset weights = {0: 1.0} ]
06/13/2020 04:44:39 AM: [ Num dev examples = 8714 ]
06/13/2020 04:44:39 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 04:44:39 AM: [ Found a checkpoint... ]
06/13/2020 04:44:39 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/13/2020 04:44:57 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 04:44:57 AM: [ Make data loaders ]
06/13/2020 04:44:57 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 04:44:57 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/13/2020 04:44:57 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 04:44:57 AM: [ Starting training... ]
06/13/2020 04:52:45 AM: [ train: Epoch 49 | perplexity = 6.76 | ml_loss = 35.58 | Time for epoch = 467.67 (s) ]
06/13/2020 04:56:41 AM: [ dev valid official: Epoch = 49 | bleu = 32.95 | rouge_l = 46.45 | Precision = 57.77 | Recall = 46.99 | F1 = 48.68 | examples = 8714 | valid time = 231.95 (s) ]
06/13/2020 04:56:41 AM: [ Best valid: bleu = 32.95 (epoch 49, 35623 updates) ]
06/13/2020 05:04:27 AM: [ train: Epoch 50 | perplexity = 6.57 | ml_loss = 35.01 | Time for epoch = 462.72 (s) ]
06/13/2020 05:08:18 AM: [ dev valid official: Epoch = 50 | bleu = 33.07 | rouge_l = 46.39 | Precision = 56.69 | Recall = 47.49 | F1 = 48.58 | examples = 8714 | valid time = 226.94 (s) ]
06/13/2020 05:08:18 AM: [ Best valid: bleu = 33.07 (epoch 50, 36350 updates) ]
06/13/2020 07:11:10 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/13/2020 07:11:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:11:11 AM: [ Load and process data files ]
06/13/2020 07:11:18 AM: [ Num train examples = 69708 ]
06/13/2020 07:11:18 AM: [ Dataset weights = {0: 1.0} ]
06/13/2020 07:11:20 AM: [ Num dev examples = 8714 ]
06/13/2020 07:11:20 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:11:20 AM: [ Found a checkpoint... ]
06/13/2020 07:11:20 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/13/2020 07:11:43 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:11:43 AM: [ Make data loaders ]
06/13/2020 07:11:43 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:11:43 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/13/2020 07:11:43 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:11:43 AM: [ Starting training... ]
06/13/2020 07:18:36 AM: [ train: Epoch 51 | perplexity = 4.77 | ml_loss = 29.20 | Time for epoch = 412.51 (s) ]
06/13/2020 07:21:15 AM: [ dev valid official: Epoch = 51 | bleu = 33.47 | rouge_l = 46.71 | Precision = 57.83 | Recall = 47.36 | F1 = 48.98 | examples = 8714 | valid time = 156.15 (s) ]
06/13/2020 07:21:15 AM: [ Best valid: bleu = 33.47 (epoch 51, 37077 updates) ]
06/13/2020 07:28:11 AM: [ train: Epoch 52 | perplexity = 4.67 | ml_loss = 28.71 | Time for epoch = 409.83 (s) ]
06/13/2020 07:30:51 AM: [ dev valid official: Epoch = 52 | bleu = 33.44 | rouge_l = 46.44 | Precision = 56.01 | Recall = 47.96 | F1 = 48.61 | examples = 8714 | valid time = 157.04 (s) ]
06/13/2020 08:45:43 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/13/2020 08:45:43 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 08:45:43 AM: [ Load and process data files ]
06/13/2020 08:45:50 AM: [ Num train examples = 69708 ]
06/13/2020 08:45:50 AM: [ Dataset weights = {0: 1.0} ]
06/13/2020 08:45:51 AM: [ Num dev examples = 8714 ]
06/13/2020 08:45:52 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 08:45:52 AM: [ Found a checkpoint... ]
06/13/2020 08:45:52 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/13/2020 08:46:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 08:46:11 AM: [ Make data loaders ]
06/13/2020 08:46:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 08:46:11 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/13/2020 08:46:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 08:46:11 AM: [ Starting training... ]
06/13/2020 08:52:58 AM: [ train: Epoch 53 | perplexity = 3.66 | ml_loss = 24.25 | Time for epoch = 406.96 (s) ]
06/13/2020 08:55:32 AM: [ dev valid official: Epoch = 53 | bleu = 33.43 | rouge_l = 46.32 | Precision = 56.71 | Recall = 47.17 | F1 = 48.54 | examples = 8714 | valid time = 150.29 (s) ]
06/13/2020 08:55:32 AM: [ Best valid: bleu = 33.43 (epoch 53, 38531 updates) ]
06/13/2020 09:02:19 AM: [ train: Epoch 54 | perplexity = 3.65 | ml_loss = 24.00 | Time for epoch = 403.01 (s) ]
06/13/2020 09:04:53 AM: [ dev valid official: Epoch = 54 | bleu = 32.78 | rouge_l = 45.41 | Precision = 53.73 | Recall = 47.85 | F1 = 47.58 | examples = 8714 | valid time = 150.29 (s) ]
06/13/2020 09:11:39 AM: [ train: Epoch 55 | perplexity = 6.11 | ml_loss = 33.90 | Time for epoch = 405.51 (s) ]
06/13/2020 09:14:13 AM: [ dev valid official: Epoch = 55 | bleu = 33.75 | rouge_l = 46.98 | Precision = 56.34 | Recall = 48.88 | F1 = 49.14 | examples = 8714 | valid time = 150.94 (s) ]
06/13/2020 09:14:13 AM: [ Best valid: bleu = 33.75 (epoch 55, 39985 updates) ]
06/13/2020 09:21:04 AM: [ train: Epoch 56 | perplexity = 5.80 | ml_loss = 33.00 | Time for epoch = 409.40 (s) ]
06/13/2020 09:23:38 AM: [ dev valid official: Epoch = 56 | bleu = 34.08 | rouge_l = 46.83 | Precision = 56.81 | Recall = 47.84 | F1 = 48.96 | examples = 8714 | valid time = 150.44 (s) ]
06/13/2020 09:23:38 AM: [ Best valid: bleu = 34.08 (epoch 56, 40712 updates) ]
06/13/2020 09:31:23 AM: [ train: Epoch 57 | perplexity = 5.37 | ml_loss = 32.35 | Time for epoch = 463.82 (s) ]
06/13/2020 09:33:57 AM: [ dev valid official: Epoch = 57 | bleu = 33.99 | rouge_l = 46.45 | Precision = 57.84 | Recall = 46.76 | F1 = 48.58 | examples = 8714 | valid time = 150.50 (s) ]
06/13/2020 09:41:48 AM: [ train: Epoch 58 | perplexity = 5.20 | ml_loss = 31.78 | Time for epoch = 470.57 (s) ]
06/13/2020 09:44:21 AM: [ dev valid official: Epoch = 58 | bleu = 34.41 | rouge_l = 47.61 | Precision = 57.50 | Recall = 48.71 | F1 = 49.77 | examples = 8714 | valid time = 150.16 (s) ]
06/13/2020 09:44:21 AM: [ Best valid: bleu = 34.41 (epoch 58, 42166 updates) ]
06/13/2020 09:51:26 AM: [ train: Epoch 59 | perplexity = 5.17 | ml_loss = 31.24 | Time for epoch = 423.45 (s) ]
06/13/2020 09:54:00 AM: [ dev valid official: Epoch = 59 | bleu = 34.34 | rouge_l = 47.50 | Precision = 55.71 | Recall = 49.96 | F1 = 49.59 | examples = 8714 | valid time = 150.88 (s) ]
06/13/2020 10:01:29 AM: [ train: Epoch 60 | perplexity = 4.96 | ml_loss = 30.74 | Time for epoch = 449.20 (s) ]
06/13/2020 10:04:04 AM: [ dev valid official: Epoch = 60 | bleu = 35.08 | rouge_l = 47.74 | Precision = 57.98 | Recall = 48.61 | F1 = 49.88 | examples = 8714 | valid time = 150.77 (s) ]
06/13/2020 10:04:04 AM: [ Best valid: bleu = 35.08 (epoch 60, 43620 updates) ]
06/13/2020 10:11:36 AM: [ train: Epoch 61 | perplexity = 4.80 | ml_loss = 30.17 | Time for epoch = 450.52 (s) ]
06/13/2020 10:14:10 AM: [ dev valid official: Epoch = 61 | bleu = 34.51 | rouge_l = 47.76 | Precision = 55.12 | Recall = 50.59 | F1 = 49.76 | examples = 8714 | valid time = 151.08 (s) ]
06/13/2020 10:20:59 AM: [ train: Epoch 62 | perplexity = 4.81 | ml_loss = 29.76 | Time for epoch = 408.58 (s) ]
06/13/2020 10:23:32 AM: [ dev valid official: Epoch = 62 | bleu = 34.86 | rouge_l = 47.06 | Precision = 58.28 | Recall = 47.23 | F1 = 49.18 | examples = 8714 | valid time = 150.33 (s) ]
06/13/2020 10:31:14 AM: [ train: Epoch 63 | perplexity = 4.49 | ml_loss = 29.22 | Time for epoch = 461.96 (s) ]
06/13/2020 10:33:49 AM: [ dev valid official: Epoch = 63 | bleu = 35.22 | rouge_l = 48.04 | Precision = 57.44 | Recall = 49.40 | F1 = 50.13 | examples = 8714 | valid time = 150.51 (s) ]
06/13/2020 10:33:49 AM: [ Best valid: bleu = 35.22 (epoch 63, 45801 updates) ]
06/13/2020 10:40:48 AM: [ train: Epoch 64 | perplexity = 4.52 | ml_loss = 28.80 | Time for epoch = 418.01 (s) ]
06/13/2020 10:43:22 AM: [ dev valid official: Epoch = 64 | bleu = 35.45 | rouge_l = 48.33 | Precision = 57.28 | Recall = 50.01 | F1 = 50.40 | examples = 8714 | valid time = 150.33 (s) ]
06/13/2020 10:43:22 AM: [ Best valid: bleu = 35.45 (epoch 64, 46528 updates) ]
06/13/2020 10:50:48 AM: [ train: Epoch 65 | perplexity = 4.34 | ml_loss = 28.35 | Time for epoch = 444.51 (s) ]
06/13/2020 10:53:21 AM: [ dev valid official: Epoch = 65 | bleu = 35.47 | rouge_l = 47.82 | Precision = 58.06 | Recall = 48.63 | F1 = 49.95 | examples = 8714 | valid time = 149.76 (s) ]
06/13/2020 10:53:21 AM: [ Best valid: bleu = 35.47 (epoch 65, 47255 updates) ]
06/13/2020 11:00:35 AM: [ train: Epoch 66 | perplexity = 4.28 | ml_loss = 27.98 | Time for epoch = 432.42 (s) ]
06/13/2020 11:03:09 AM: [ dev valid official: Epoch = 66 | bleu = 35.57 | rouge_l = 48.34 | Precision = 56.34 | Recall = 50.44 | F1 = 50.40 | examples = 8714 | valid time = 150.11 (s) ]
06/13/2020 11:03:09 AM: [ Best valid: bleu = 35.57 (epoch 66, 47982 updates) ]
06/13/2020 11:10:49 AM: [ train: Epoch 67 | perplexity = 4.08 | ml_loss = 27.56 | Time for epoch = 458.88 (s) ]
06/13/2020 11:13:21 AM: [ dev valid official: Epoch = 67 | bleu = 35.99 | rouge_l = 48.32 | Precision = 57.82 | Recall = 49.42 | F1 = 50.40 | examples = 8714 | valid time = 148.61 (s) ]
06/13/2020 11:13:21 AM: [ Best valid: bleu = 35.99 (epoch 67, 48709 updates) ]
06/13/2020 11:20:24 AM: [ train: Epoch 68 | perplexity = 4.07 | ml_loss = 27.11 | Time for epoch = 421.24 (s) ]
06/13/2020 11:22:57 AM: [ dev valid official: Epoch = 68 | bleu = 36.00 | rouge_l = 48.55 | Precision = 57.23 | Recall = 50.22 | F1 = 50.63 | examples = 8714 | valid time = 149.52 (s) ]
06/13/2020 11:22:57 AM: [ Best valid: bleu = 36.00 (epoch 68, 49436 updates) ]
06/13/2020 11:29:48 AM: [ train: Epoch 69 | perplexity = 4.04 | ml_loss = 26.73 | Time for epoch = 409.09 (s) ]
06/13/2020 11:32:21 AM: [ dev valid official: Epoch = 69 | bleu = 36.24 | rouge_l = 48.52 | Precision = 58.19 | Recall = 49.58 | F1 = 50.61 | examples = 8714 | valid time = 149.62 (s) ]
06/13/2020 11:32:21 AM: [ Best valid: bleu = 36.24 (epoch 69, 50163 updates) ]
06/13/2020 11:39:14 AM: [ train: Epoch 70 | perplexity = 3.95 | ml_loss = 26.39 | Time for epoch = 411.52 (s) ]
06/13/2020 11:41:47 AM: [ dev valid official: Epoch = 70 | bleu = 36.13 | rouge_l = 48.72 | Precision = 56.34 | Recall = 51.36 | F1 = 50.71 | examples = 8714 | valid time = 149.47 (s) ]
06/13/2020 11:48:58 AM: [ train: Epoch 71 | perplexity = 3.80 | ml_loss = 26.01 | Time for epoch = 431.40 (s) ]
06/13/2020 11:51:32 AM: [ dev valid official: Epoch = 71 | bleu = 36.19 | rouge_l = 48.53 | Precision = 55.75 | Recall = 51.17 | F1 = 50.55 | examples = 8714 | valid time = 149.82 (s) ]
06/13/2020 11:58:57 AM: [ train: Epoch 72 | perplexity = 3.71 | ml_loss = 25.65 | Time for epoch = 445.30 (s) ]
06/13/2020 12:01:30 PM: [ dev valid official: Epoch = 72 | bleu = 36.57 | rouge_l = 48.89 | Precision = 57.76 | Recall = 50.31 | F1 = 50.91 | examples = 8714 | valid time = 150.14 (s) ]
06/13/2020 12:01:30 PM: [ Best valid: bleu = 36.57 (epoch 72, 52344 updates) ]
06/13/2020 12:08:18 PM: [ train: Epoch 73 | perplexity = 3.73 | ml_loss = 25.32 | Time for epoch = 406.67 (s) ]
06/13/2020 12:10:53 PM: [ dev valid official: Epoch = 73 | bleu = 36.49 | rouge_l = 48.96 | Precision = 56.33 | Recall = 51.66 | F1 = 51.03 | examples = 8714 | valid time = 150.51 (s) ]
06/13/2020 12:18:40 PM: [ train: Epoch 74 | perplexity = 3.51 | ml_loss = 24.98 | Time for epoch = 466.82 (s) ]
06/13/2020 12:21:14 PM: [ dev valid official: Epoch = 74 | bleu = 36.93 | rouge_l = 49.06 | Precision = 57.05 | Recall = 51.31 | F1 = 51.18 | examples = 8714 | valid time = 150.94 (s) ]
06/13/2020 12:21:14 PM: [ Best valid: bleu = 36.93 (epoch 74, 53798 updates) ]
06/13/2020 12:28:14 PM: [ train: Epoch 75 | perplexity = 3.55 | ml_loss = 24.61 | Time for epoch = 419.04 (s) ]
06/13/2020 12:30:49 PM: [ dev valid official: Epoch = 75 | bleu = 36.78 | rouge_l = 48.87 | Precision = 56.76 | Recall = 51.05 | F1 = 50.87 | examples = 8714 | valid time = 150.96 (s) ]
06/13/2020 12:37:32 PM: [ train: Epoch 76 | perplexity = 3.51 | ml_loss = 24.29 | Time for epoch = 402.72 (s) ]
06/13/2020 12:40:05 PM: [ dev valid official: Epoch = 76 | bleu = 37.10 | rouge_l = 49.11 | Precision = 57.25 | Recall = 50.97 | F1 = 51.17 | examples = 8714 | valid time = 149.60 (s) ]
06/13/2020 12:40:05 PM: [ Best valid: bleu = 37.10 (epoch 76, 55252 updates) ]
06/13/2020 12:47:19 PM: [ train: Epoch 77 | perplexity = 3.38 | ml_loss = 23.98 | Time for epoch = 432.69 (s) ]
06/13/2020 12:49:54 PM: [ dev valid official: Epoch = 77 | bleu = 36.99 | rouge_l = 49.10 | Precision = 56.55 | Recall = 51.67 | F1 = 51.16 | examples = 8714 | valid time = 151.56 (s) ]
06/13/2020 12:56:36 PM: [ train: Epoch 78 | perplexity = 3.40 | ml_loss = 23.68 | Time for epoch = 401.98 (s) ]
06/13/2020 12:59:13 PM: [ dev valid official: Epoch = 78 | bleu = 37.35 | rouge_l = 49.55 | Precision = 57.49 | Recall = 51.70 | F1 = 51.65 | examples = 8714 | valid time = 153.43 (s) ]
06/13/2020 12:59:13 PM: [ Best valid: bleu = 37.35 (epoch 78, 56706 updates) ]
06/13/2020 01:06:19 PM: [ train: Epoch 79 | perplexity = 3.30 | ml_loss = 23.36 | Time for epoch = 423.45 (s) ]
06/13/2020 01:08:58 PM: [ dev valid official: Epoch = 79 | bleu = 37.33 | rouge_l = 49.37 | Precision = 56.91 | Recall = 51.93 | F1 = 51.45 | examples = 8714 | valid time = 155.68 (s) ]
06/13/2020 01:19:10 PM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/13/2020 01:19:11 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 01:19:11 PM: [ Load and process data files ]
06/13/2020 01:19:19 PM: [ Num train examples = 69708 ]
06/13/2020 01:19:19 PM: [ Dataset weights = {0: 1.0} ]
06/13/2020 01:19:20 PM: [ Num dev examples = 8714 ]
06/13/2020 01:19:20 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 01:19:20 PM: [ Found a checkpoint... ]
06/13/2020 01:19:20 PM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/13/2020 01:19:29 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 01:19:29 PM: [ Make data loaders ]
06/13/2020 01:19:29 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 01:19:29 PM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": false,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/13/2020 01:19:29 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 01:19:29 PM: [ Starting training... ]
06/13/2020 07:14:18 PM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/13/2020 07:14:19 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:14:19 PM: [ Load and process data files ]
06/13/2020 07:14:30 PM: [ Num train examples = 69708 ]
06/13/2020 07:14:30 PM: [ Dataset weights = {0: 1.0} ]
06/13/2020 07:14:34 PM: [ Num dev examples = 8714 ]
06/13/2020 07:14:34 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:14:34 PM: [ Found a checkpoint... ]
06/13/2020 07:14:34 PM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/13/2020 07:14:55 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:14:55 PM: [ Make data loaders ]
06/13/2020 07:14:55 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:14:55 PM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/13/2020 07:14:55 PM: [ ---------------------------------------------------------------------------------------------------- ]
06/13/2020 07:14:55 PM: [ Starting training... ]
06/13/2020 07:26:09 PM: [ train: Epoch 80 | perplexity = 3.44 | ml_loss = 23.98 | Time for epoch = 674.27 (s) ]
06/13/2020 07:29:12 PM: [ dev valid official: Epoch = 80 | bleu = 37.41 | rouge_l = 49.24 | Precision = 57.78 | Recall = 50.76 | F1 = 51.27 | examples = 8714 | valid time = 178.37 (s) ]
06/13/2020 07:29:12 PM: [ Best valid: bleu = 37.41 (epoch 80, 58160 updates) ]
06/13/2020 07:40:28 PM: [ train: Epoch 81 | perplexity = 3.39 | ml_loss = 23.70 | Time for epoch = 671.62 (s) ]
06/13/2020 07:43:26 PM: [ dev valid official: Epoch = 81 | bleu = 37.40 | rouge_l = 49.23 | Precision = 57.10 | Recall = 51.45 | F1 = 51.30 | examples = 8714 | valid time = 173.91 (s) ]
06/13/2020 07:54:41 PM: [ train: Epoch 82 | perplexity = 3.32 | ml_loss = 23.42 | Time for epoch = 675.35 (s) ]
06/13/2020 07:57:37 PM: [ dev valid official: Epoch = 82 | bleu = 37.62 | rouge_l = 49.58 | Precision = 56.84 | Recall = 52.29 | F1 = 51.61 | examples = 8714 | valid time = 172.41 (s) ]
06/13/2020 07:57:37 PM: [ Best valid: bleu = 37.62 (epoch 82, 59614 updates) ]
06/13/2020 08:08:59 PM: [ train: Epoch 83 | perplexity = 3.27 | ml_loss = 23.15 | Time for epoch = 680.67 (s) ]
06/13/2020 08:11:57 PM: [ dev valid official: Epoch = 83 | bleu = 37.89 | rouge_l = 49.75 | Precision = 57.55 | Recall = 51.77 | F1 = 51.76 | examples = 8714 | valid time = 174.04 (s) ]
06/13/2020 08:11:57 PM: [ Best valid: bleu = 37.89 (epoch 83, 60341 updates) ]
06/13/2020 08:25:07 PM: [ train: Epoch 84 | perplexity = 3.11 | ml_loss = 22.91 | Time for epoch = 788.96 (s) ]
06/13/2020 08:28:01 PM: [ dev valid official: Epoch = 84 | bleu = 38.03 | rouge_l = 49.34 | Precision = 58.50 | Recall = 50.44 | F1 = 51.37 | examples = 8714 | valid time = 170.98 (s) ]
06/13/2020 08:28:01 PM: [ Best valid: bleu = 38.03 (epoch 84, 61068 updates) ]
06/13/2020 08:41:25 PM: [ train: Epoch 85 | perplexity = 3.06 | ml_loss = 22.62 | Time for epoch = 802.63 (s) ]
06/13/2020 08:44:20 PM: [ dev valid official: Epoch = 85 | bleu = 38.04 | rouge_l = 49.84 | Precision = 57.82 | Recall = 51.71 | F1 = 51.90 | examples = 8714 | valid time = 171.21 (s) ]
06/13/2020 08:44:20 PM: [ Best valid: bleu = 38.04 (epoch 85, 61795 updates) ]
06/13/2020 08:56:06 PM: [ train: Epoch 86 | perplexity = 3.08 | ml_loss = 22.36 | Time for epoch = 705.28 (s) ]
06/13/2020 08:59:01 PM: [ dev valid official: Epoch = 86 | bleu = 37.77 | rouge_l = 49.50 | Precision = 56.13 | Recall = 52.71 | F1 = 51.56 | examples = 8714 | valid time = 170.56 (s) ]
06/13/2020 09:11:39 PM: [ train: Epoch 87 | perplexity = 3.01 | ml_loss = 22.08 | Time for epoch = 758.37 (s) ]
06/13/2020 09:14:32 PM: [ dev valid official: Epoch = 87 | bleu = 38.24 | rouge_l = 49.84 | Precision = 57.79 | Recall = 51.69 | F1 = 51.88 | examples = 8714 | valid time = 169.11 (s) ]
06/13/2020 09:14:32 PM: [ Best valid: bleu = 38.24 (epoch 87, 63249 updates) ]
06/13/2020 09:27:13 PM: [ train: Epoch 88 | perplexity = 2.95 | ml_loss = 21.81 | Time for epoch = 760.36 (s) ]
06/13/2020 09:30:07 PM: [ dev valid official: Epoch = 88 | bleu = 37.76 | rouge_l = 49.41 | Precision = 55.90 | Recall = 52.48 | F1 = 51.38 | examples = 8714 | valid time = 169.63 (s) ]
06/13/2020 09:41:21 PM: [ train: Epoch 89 | perplexity = 2.99 | ml_loss = 21.60 | Time for epoch = 674.28 (s) ]
06/13/2020 09:44:15 PM: [ dev valid official: Epoch = 89 | bleu = 38.49 | rouge_l = 49.95 | Precision = 58.96 | Recall = 51.07 | F1 = 52.01 | examples = 8714 | valid time = 169.79 (s) ]
06/13/2020 09:44:15 PM: [ Best valid: bleu = 38.49 (epoch 89, 64703 updates) ]
06/13/2020 09:57:18 PM: [ train: Epoch 90 | perplexity = 2.85 | ml_loss = 21.35 | Time for epoch = 781.75 (s) ]
06/13/2020 10:00:13 PM: [ dev valid official: Epoch = 90 | bleu = 38.63 | rouge_l = 50.33 | Precision = 58.25 | Recall = 52.14 | F1 = 52.33 | examples = 8714 | valid time = 171.87 (s) ]
06/13/2020 10:00:13 PM: [ Best valid: bleu = 38.63 (epoch 90, 65430 updates) ]
06/13/2020 10:11:51 PM: [ train: Epoch 91 | perplexity = 2.89 | ml_loss = 21.11 | Time for epoch = 696.61 (s) ]
06/13/2020 10:14:50 PM: [ dev valid official: Epoch = 91 | bleu = 38.42 | rouge_l = 50.15 | Precision = 57.04 | Recall = 52.81 | F1 = 52.12 | examples = 8714 | valid time = 175.75 (s) ]
06/13/2020 10:27:20 PM: [ train: Epoch 92 | perplexity = 2.82 | ml_loss = 20.90 | Time for epoch = 749.35 (s) ]
06/13/2020 10:30:16 PM: [ dev valid official: Epoch = 92 | bleu = 38.59 | rouge_l = 49.86 | Precision = 58.36 | Recall = 51.24 | F1 = 51.88 | examples = 8714 | valid time = 172.29 (s) ]
06/13/2020 10:42:11 PM: [ train: Epoch 93 | perplexity = 2.80 | ml_loss = 20.69 | Time for epoch = 715.64 (s) ]
06/13/2020 10:45:06 PM: [ dev valid official: Epoch = 93 | bleu = 38.66 | rouge_l = 50.28 | Precision = 57.09 | Recall = 52.93 | F1 = 52.30 | examples = 8714 | valid time = 171.22 (s) ]
06/13/2020 10:45:06 PM: [ Best valid: bleu = 38.66 (epoch 93, 67611 updates) ]
06/13/2020 10:58:08 PM: [ train: Epoch 94 | perplexity = 2.72 | ml_loss = 20.46 | Time for epoch = 780.65 (s) ]
06/13/2020 11:01:09 PM: [ dev valid official: Epoch = 94 | bleu = 38.93 | rouge_l = 50.23 | Precision = 58.45 | Recall = 51.69 | F1 = 52.21 | examples = 8714 | valid time = 176.81 (s) ]
06/13/2020 11:01:09 PM: [ Best valid: bleu = 38.93 (epoch 94, 68338 updates) ]
06/13/2020 11:13:01 PM: [ train: Epoch 95 | perplexity = 2.73 | ml_loss = 20.21 | Time for epoch = 710.48 (s) ]
06/13/2020 11:16:02 PM: [ dev valid official: Epoch = 95 | bleu = 39.05 | rouge_l = 50.58 | Precision = 57.79 | Recall = 53.06 | F1 = 52.63 | examples = 8714 | valid time = 177.11 (s) ]
06/13/2020 11:16:02 PM: [ Best valid: bleu = 39.05 (epoch 95, 69065 updates) ]
06/13/2020 11:27:27 PM: [ train: Epoch 96 | perplexity = 2.73 | ml_loss = 20.00 | Time for epoch = 684.11 (s) ]
06/13/2020 11:30:24 PM: [ dev valid official: Epoch = 96 | bleu = 39.00 | rouge_l = 50.47 | Precision = 58.23 | Recall = 52.36 | F1 = 52.46 | examples = 8714 | valid time = 172.98 (s) ]
06/13/2020 11:41:38 PM: [ train: Epoch 97 | perplexity = 2.69 | ml_loss = 19.84 | Time for epoch = 673.71 (s) ]
06/13/2020 11:44:38 PM: [ dev valid official: Epoch = 97 | bleu = 38.75 | rouge_l = 50.31 | Precision = 56.50 | Recall = 53.62 | F1 = 52.25 | examples = 8714 | valid time = 176.41 (s) ]
06/13/2020 11:56:48 PM: [ train: Epoch 98 | perplexity = 2.63 | ml_loss = 19.62 | Time for epoch = 729.96 (s) ]
06/13/2020 11:59:50 PM: [ dev valid official: Epoch = 98 | bleu = 38.94 | rouge_l = 50.42 | Precision = 56.84 | Recall = 53.46 | F1 = 52.43 | examples = 8714 | valid time = 179.10 (s) ]
06/14/2020 12:12:29 AM: [ train: Epoch 99 | perplexity = 2.59 | ml_loss = 19.40 | Time for epoch = 759.10 (s) ]
06/14/2020 12:15:30 AM: [ dev valid official: Epoch = 99 | bleu = 39.21 | rouge_l = 50.57 | Precision = 58.74 | Recall = 52.30 | F1 = 52.59 | examples = 8714 | valid time = 176.51 (s) ]
06/14/2020 12:15:30 AM: [ Best valid: bleu = 39.21 (epoch 99, 71973 updates) ]
06/14/2020 12:26:54 AM: [ train: Epoch 100 | perplexity = 2.61 | ml_loss = 19.23 | Time for epoch = 683.40 (s) ]
06/14/2020 12:29:57 AM: [ dev valid official: Epoch = 100 | bleu = 39.06 | rouge_l = 50.47 | Precision = 56.88 | Recall = 53.50 | F1 = 52.44 | examples = 8714 | valid time = 179.54 (s) ]
06/14/2020 12:43:21 AM: [ train: Epoch 101 | perplexity = 2.50 | ml_loss = 19.04 | Time for epoch = 803.71 (s) ]
06/14/2020 12:46:23 AM: [ dev valid official: Epoch = 101 | bleu = 39.30 | rouge_l = 50.66 | Precision = 57.32 | Recall = 53.47 | F1 = 52.66 | examples = 8714 | valid time = 178.72 (s) ]
06/14/2020 12:46:23 AM: [ Best valid: bleu = 39.30 (epoch 101, 73427 updates) ]
06/14/2020 12:58:14 AM: [ train: Epoch 102 | perplexity = 2.54 | ml_loss = 18.85 | Time for epoch = 708.54 (s) ]
06/14/2020 01:01:16 AM: [ dev valid official: Epoch = 102 | bleu = 39.33 | rouge_l = 50.68 | Precision = 57.66 | Recall = 53.21 | F1 = 52.67 | examples = 8714 | valid time = 179.07 (s) ]
06/14/2020 01:01:16 AM: [ Best valid: bleu = 39.33 (epoch 102, 74154 updates) ]
06/14/2020 01:12:32 AM: [ train: Epoch 103 | perplexity = 2.52 | ml_loss = 18.64 | Time for epoch = 674.31 (s) ]
06/14/2020 01:15:34 AM: [ dev valid official: Epoch = 103 | bleu = 39.43 | rouge_l = 50.89 | Precision = 57.86 | Recall = 53.32 | F1 = 52.87 | examples = 8714 | valid time = 178.13 (s) ]
06/14/2020 01:15:34 AM: [ Best valid: bleu = 39.43 (epoch 103, 74881 updates) ]
06/14/2020 01:27:51 AM: [ train: Epoch 104 | perplexity = 2.47 | ml_loss = 18.49 | Time for epoch = 736.24 (s) ]
06/14/2020 01:30:52 AM: [ dev valid official: Epoch = 104 | bleu = 39.24 | rouge_l = 50.64 | Precision = 56.75 | Recall = 53.97 | F1 = 52.60 | examples = 8714 | valid time = 177.18 (s) ]
06/14/2020 01:42:02 AM: [ train: Epoch 105 | perplexity = 2.48 | ml_loss = 18.29 | Time for epoch = 670.29 (s) ]
06/14/2020 01:45:06 AM: [ dev valid official: Epoch = 105 | bleu = 39.51 | rouge_l = 50.93 | Precision = 57.40 | Recall = 53.71 | F1 = 52.88 | examples = 8714 | valid time = 180.02 (s) ]
06/14/2020 01:45:06 AM: [ Best valid: bleu = 39.51 (epoch 105, 76335 updates) ]
06/14/2020 01:56:56 AM: [ train: Epoch 106 | perplexity = 2.44 | ml_loss = 18.16 | Time for epoch = 708.89 (s) ]
06/14/2020 01:59:58 AM: [ dev valid official: Epoch = 106 | bleu = 39.61 | rouge_l = 51.02 | Precision = 57.70 | Recall = 53.80 | F1 = 52.99 | examples = 8714 | valid time = 177.92 (s) ]
06/14/2020 01:59:58 AM: [ Best valid: bleu = 39.61 (epoch 106, 77062 updates) ]
06/14/2020 02:13:03 AM: [ train: Epoch 107 | perplexity = 2.35 | ml_loss = 17.91 | Time for epoch = 783.70 (s) ]
06/14/2020 02:16:05 AM: [ dev valid official: Epoch = 107 | bleu = 39.79 | rouge_l = 51.11 | Precision = 58.13 | Recall = 53.46 | F1 = 53.10 | examples = 8714 | valid time = 178.86 (s) ]
06/14/2020 02:16:05 AM: [ Best valid: bleu = 39.79 (epoch 107, 77789 updates) ]
06/14/2020 02:27:21 AM: [ train: Epoch 108 | perplexity = 2.41 | ml_loss = 17.79 | Time for epoch = 674.37 (s) ]
06/14/2020 02:30:27 AM: [ dev valid official: Epoch = 108 | bleu = 39.32 | rouge_l = 50.80 | Precision = 55.82 | Recall = 55.14 | F1 = 52.75 | examples = 8714 | valid time = 181.61 (s) ]
06/14/2020 02:43:28 AM: [ train: Epoch 109 | perplexity = 2.31 | ml_loss = 17.62 | Time for epoch = 780.91 (s) ]
06/14/2020 02:46:31 AM: [ dev valid official: Epoch = 109 | bleu = 39.75 | rouge_l = 51.13 | Precision = 56.75 | Recall = 54.71 | F1 = 53.09 | examples = 8714 | valid time = 179.34 (s) ]
06/14/2020 02:58:33 AM: [ train: Epoch 110 | perplexity = 2.33 | ml_loss = 17.45 | Time for epoch = 721.90 (s) ]
06/14/2020 03:01:36 AM: [ dev valid official: Epoch = 110 | bleu = 39.72 | rouge_l = 50.98 | Precision = 57.96 | Recall = 53.36 | F1 = 52.97 | examples = 8714 | valid time = 179.08 (s) ]
06/14/2020 03:12:54 AM: [ train: Epoch 111 | perplexity = 2.33 | ml_loss = 17.31 | Time for epoch = 677.79 (s) ]
06/14/2020 03:15:57 AM: [ dev valid official: Epoch = 111 | bleu = 40.04 | rouge_l = 51.05 | Precision = 58.76 | Recall = 52.78 | F1 = 53.00 | examples = 8714 | valid time = 179.40 (s) ]
06/14/2020 03:15:57 AM: [ Best valid: bleu = 40.04 (epoch 111, 80697 updates) ]
06/14/2020 03:27:57 AM: [ train: Epoch 112 | perplexity = 2.30 | ml_loss = 17.13 | Time for epoch = 718.43 (s) ]
06/14/2020 03:31:02 AM: [ dev valid official: Epoch = 112 | bleu = 39.71 | rouge_l = 50.93 | Precision = 56.68 | Recall = 54.43 | F1 = 52.86 | examples = 8714 | valid time = 181.46 (s) ]
06/14/2020 03:42:35 AM: [ train: Epoch 113 | perplexity = 2.28 | ml_loss = 16.98 | Time for epoch = 693.68 (s) ]
06/14/2020 03:45:43 AM: [ dev valid official: Epoch = 113 | bleu = 40.32 | rouge_l = 51.49 | Precision = 59.22 | Recall = 53.19 | F1 = 53.42 | examples = 8714 | valid time = 184.12 (s) ]
06/14/2020 03:45:43 AM: [ Best valid: bleu = 40.32 (epoch 113, 82151 updates) ]
06/14/2020 03:57:53 AM: [ train: Epoch 114 | perplexity = 2.25 | ml_loss = 16.86 | Time for epoch = 728.87 (s) ]
06/14/2020 04:00:57 AM: [ dev valid official: Epoch = 114 | bleu = 40.01 | rouge_l = 51.04 | Precision = 57.17 | Recall = 54.11 | F1 = 52.99 | examples = 8714 | valid time = 180.19 (s) ]
06/14/2020 04:13:57 AM: [ train: Epoch 115 | perplexity = 2.20 | ml_loss = 16.64 | Time for epoch = 780.02 (s) ]
06/14/2020 04:16:59 AM: [ dev valid official: Epoch = 115 | bleu = 40.06 | rouge_l = 51.11 | Precision = 57.10 | Recall = 54.42 | F1 = 53.06 | examples = 8714 | valid time = 177.72 (s) ]
06/14/2020 04:29:05 AM: [ train: Epoch 116 | perplexity = 2.21 | ml_loss = 16.53 | Time for epoch = 726.12 (s) ]
06/14/2020 04:32:07 AM: [ dev valid official: Epoch = 116 | bleu = 40.13 | rouge_l = 51.15 | Precision = 57.54 | Recall = 54.07 | F1 = 53.12 | examples = 8714 | valid time = 178.37 (s) ]
06/14/2020 04:44:20 AM: [ train: Epoch 117 | perplexity = 2.18 | ml_loss = 16.36 | Time for epoch = 733.29 (s) ]
06/14/2020 04:47:22 AM: [ dev valid official: Epoch = 117 | bleu = 40.24 | rouge_l = 51.38 | Precision = 57.79 | Recall = 54.19 | F1 = 53.34 | examples = 8714 | valid time = 178.33 (s) ]
06/14/2020 05:00:34 AM: [ train: Epoch 118 | perplexity = 2.14 | ml_loss = 16.23 | Time for epoch = 791.69 (s) ]
06/14/2020 05:03:36 AM: [ dev valid official: Epoch = 118 | bleu = 40.22 | rouge_l = 51.27 | Precision = 57.44 | Recall = 54.40 | F1 = 53.19 | examples = 8714 | valid time = 178.74 (s) ]
06/20/2020 06:15:30 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/20/2020 06:15:30 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:15:30 AM: [ Load and process data files ]
06/20/2020 06:15:43 AM: [ Num train examples = 69708 ]
06/20/2020 06:15:43 AM: [ Dataset weights = {0: 1.0} ]
06/20/2020 06:15:46 AM: [ Num dev examples = 8714 ]
06/20/2020 06:15:46 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:15:46 AM: [ Found a checkpoint... ]
06/20/2020 06:15:46 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/20/2020 06:16:04 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:16:04 AM: [ Make data loaders ]
06/20/2020 06:16:04 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:16:04 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/20/2020 06:16:04 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:16:04 AM: [ Starting training... ]
06/20/2020 06:17:45 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 96 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/20/2020 06:17:45 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:17:45 AM: [ Load and process data files ]
06/20/2020 06:17:51 AM: [ Num train examples = 69708 ]
06/20/2020 06:17:51 AM: [ Dataset weights = {0: 1.0} ]
06/20/2020 06:17:53 AM: [ Num dev examples = 8714 ]
06/20/2020 06:17:53 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:17:53 AM: [ Found a checkpoint... ]
06/20/2020 06:17:53 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/20/2020 06:17:58 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:17:58 AM: [ Make data loaders ]
06/20/2020 06:17:58 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:17:58 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 96,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/20/2020 06:17:58 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:17:58 AM: [ Starting training... ]
06/20/2020 06:19:57 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 64 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/20/2020 06:19:57 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:19:57 AM: [ Load and process data files ]
06/20/2020 06:20:04 AM: [ Num train examples = 69708 ]
06/20/2020 06:20:04 AM: [ Dataset weights = {0: 1.0} ]
06/20/2020 06:20:05 AM: [ Num dev examples = 8714 ]
06/20/2020 06:20:05 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:20:05 AM: [ Found a checkpoint... ]
06/20/2020 06:20:05 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/20/2020 06:20:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:20:11 AM: [ Make data loaders ]
06/20/2020 06:20:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:20:11 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 64,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/20/2020 06:20:11 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 06:20:11 AM: [ Starting training... ]
06/20/2020 06:39:52 AM: [ train: Epoch 119 | perplexity = 2.22 | ml_loss = 16.31 | Time for epoch = 1180.68 (s) ]
06/20/2020 06:45:37 AM: [ dev valid official: Epoch = 119 | bleu = 40.36 | rouge_l = 51.33 | Precision = 58.56 | Recall = 53.30 | F1 = 53.29 | examples = 8714 | valid time = 341.07 (s) ]
06/20/2020 06:45:37 AM: [ Best valid: bleu = 40.36 (epoch 119, 86876 updates) ]
06/20/2020 07:08:14 AM: [ train: Epoch 120 | perplexity = 2.15 | ml_loss = 16.26 | Time for epoch = 1356.35 (s) ]
06/20/2020 07:13:57 AM: [ dev valid official: Epoch = 120 | bleu = 40.40 | rouge_l = 51.44 | Precision = 58.26 | Recall = 53.82 | F1 = 53.35 | examples = 8714 | valid time = 338.42 (s) ]
06/20/2020 07:13:57 AM: [ Best valid: bleu = 40.40 (epoch 120, 87966 updates) ]
06/20/2020 07:36:59 AM: [ train: Epoch 121 | perplexity = 2.11 | ml_loss = 16.10 | Time for epoch = 1381.43 (s) ]
06/20/2020 07:42:45 AM: [ dev valid official: Epoch = 121 | bleu = 40.16 | rouge_l = 51.32 | Precision = 57.00 | Recall = 54.82 | F1 = 53.28 | examples = 8714 | valid time = 341.58 (s) ]
06/20/2020 08:02:14 AM: [ train: Epoch 122 | perplexity = 2.17 | ml_loss = 15.94 | Time for epoch = 1169.03 (s) ]
06/20/2020 08:08:00 AM: [ dev valid official: Epoch = 122 | bleu = 40.50 | rouge_l = 51.66 | Precision = 58.31 | Recall = 54.11 | F1 = 53.57 | examples = 8714 | valid time = 342.41 (s) ]
06/20/2020 08:08:00 AM: [ Best valid: bleu = 40.50 (epoch 122, 90146 updates) ]
06/20/2020 09:32:21 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 64 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/20/2020 09:32:22 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 09:32:22 AM: [ Load and process data files ]
06/20/2020 09:32:34 AM: [ Num train examples = 69708 ]
06/20/2020 09:32:34 AM: [ Dataset weights = {0: 1.0} ]
06/20/2020 09:32:39 AM: [ Num dev examples = 8714 ]
06/20/2020 09:32:39 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 09:32:39 AM: [ Found a checkpoint... ]
06/20/2020 09:32:39 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/20/2020 09:33:13 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 09:33:13 AM: [ Make data loaders ]
06/20/2020 09:33:13 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 09:33:13 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 64,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/20/2020 09:33:13 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/20/2020 09:33:13 AM: [ Starting training... ]
06/20/2020 09:43:30 AM: [ train: Epoch 123 | perplexity = 2.15 | ml_loss = 15.83 | Time for epoch = 616.96 (s) ]
06/20/2020 09:46:25 AM: [ dev valid official: Epoch = 123 | bleu = 40.66 | rouge_l = 51.63 | Precision = 58.45 | Recall = 53.86 | F1 = 53.56 | examples = 8714 | valid time = 171.87 (s) ]
06/20/2020 09:46:25 AM: [ Best valid: bleu = 40.66 (epoch 123, 91236 updates) ]
06/20/2020 09:58:24 AM: [ train: Epoch 124 | perplexity = 2.08 | ml_loss = 15.65 | Time for epoch = 709.09 (s) ]
06/20/2020 10:01:23 AM: [ dev valid official: Epoch = 124 | bleu = 40.60 | rouge_l = 51.50 | Precision = 57.96 | Recall = 54.18 | F1 = 53.47 | examples = 8714 | valid time = 175.05 (s) ]
06/20/2020 10:13:27 AM: [ train: Epoch 125 | perplexity = 2.05 | ml_loss = 15.54 | Time for epoch = 723.96 (s) ]
06/20/2020 10:16:26 AM: [ dev valid official: Epoch = 125 | bleu = 40.33 | rouge_l = 51.48 | Precision = 57.44 | Recall = 54.71 | F1 = 53.41 | examples = 8714 | valid time = 175.75 (s) ]
06/24/2020 11:49:15 AM: [ COMMAND: ../../main/train.py --data_workers 5 --dataset_name java --data_dir ../../data/ --model_dir ../../tmp --model_name code2jdoc --train_src train/code.original_subtoken --train_src_tag train/code.original_subtoken --train_tgt train/javadoc.original --dev_src dev/code.original_subtoken --dev_src_tag dev/code.original_subtoken --dev_tgt dev/javadoc.original --code_tag_type original_subtoken --use_code_type False --uncase True --use_src_word True --use_src_char False --use_tgt_word True --use_tgt_char False --max_src_len 150 --max_tgt_len 50 --emsize 512 --fix_embeddings False --src_vocab_size 50000 --tgt_vocab_size 30000 --share_decoder_embeddings True --max_examples -1 --batch_size 64 --test_batch_size 64 --num_epochs 200 --model_type transformer --num_head 8 --d_k 64 --d_v 64 --d_ff 2048 --src_pos_emb False --tgt_pos_emb True --max_relative_pos 32 --use_neg_dist True --nlayers 6 --trans_drop 0.2 --dropout_emb 0.2 --dropout 0.2 --copy_attn True --early_stop 20 --warmup_steps 2000 --optimizer adam --learning_rate 0.0001 --lr_decay 0.99 --valid_metric bleu --checkpoint True ]
06/24/2020 11:49:15 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/24/2020 11:49:15 AM: [ Load and process data files ]
06/24/2020 11:49:29 AM: [ Num train examples = 69708 ]
06/24/2020 11:49:29 AM: [ Dataset weights = {0: 1.0} ]
06/24/2020 11:49:33 AM: [ Num dev examples = 8714 ]
06/24/2020 11:49:33 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/24/2020 11:49:33 AM: [ Found a checkpoint... ]
06/24/2020 11:49:33 AM: [ Loading model ../../tmp/code2jdoc.mdl.checkpoint ]
06/24/2020 11:49:55 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/24/2020 11:49:55 AM: [ Make data loaders ]
06/24/2020 11:49:55 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/24/2020 11:49:55 AM: [ CONFIG:
{
    "attn_type": "general",
    "batch_size": 64,
    "bidirection": true,
    "char_emsize": 16,
    "checkpoint": true,
    "code_tag_type": "original_subtoken",
    "conditional_decoding": false,
    "copy_attn": true,
    "coverage_attn": false,
    "cuda": true,
    "d_ff": 2048,
    "d_k": 64,
    "d_v": 64,
    "data_dir": "../../data/",
    "data_workers": 5,
    "dataset_name": [
        "java"
    ],
    "dataset_weights": {
        "0": 1.0
    },
    "dev_src": [
        "dev/code.original_subtoken"
    ],
    "dev_src_files": [
        "../../data/java/dev/code.original_subtoken"
    ],
    "dev_src_tag": [
        "dev/code.original_subtoken"
    ],
    "dev_src_tag_files": [
        null
    ],
    "dev_tgt": [
        "dev/javadoc.original"
    ],
    "dev_tgt_files": [
        "../../data/java/dev/javadoc.original"
    ],
    "display_iter": 25,
    "dropout": 0.2,
    "dropout_emb": 0.2,
    "dropout_rnn": 0.2,
    "early_stop": 20,
    "emsize": 512,
    "filter_size": 5,
    "fix_embeddings": false,
    "force_copy": false,
    "grad_clipping": 5.0,
    "layer_wise_attn": false,
    "learning_rate": 0.0001,
    "log_file": "../../tmp/code2jdoc.txt",
    "lr_decay": 0.99,
    "max_characters_per_token": 30,
    "max_examples": -1,
    "max_relative_pos": [
        32
    ],
    "max_src_len": 150,
    "max_tgt_len": 50,
    "model_dir": "../../tmp",
    "model_file": "../../tmp/code2jdoc.mdl",
    "model_name": "code2jdoc",
    "model_type": "transformer",
    "momentum": 0,
    "n_characters": 260,
    "nfilters": 100,
    "nhid": 200,
    "nlayers": 6,
    "num_epochs": 200,
    "num_head": 8,
    "num_train_examples": 69708,
    "only_test": false,
    "optimizer": "adam",
    "parallel": false,
    "pred_file": "../../tmp/code2jdoc.json",
    "pretrained": null,
    "print_copy_info": false,
    "print_one_target": false,
    "random_seed": 1013,
    "reload_decoder_state": null,
    "reuse_copy_attn": false,
    "review_attn": false,
    "rnn_type": "LSTM",
    "share_decoder_embeddings": true,
    "sort_by_len": true,
    "split_decoder": false,
    "src_pos_emb": false,
    "src_vocab_size": 50000,
    "test_batch_size": 64,
    "tgt_pos_emb": true,
    "tgt_vocab_size": 30000,
    "train_src": [
        "train/code.original_subtoken"
    ],
    "train_src_files": [
        "../../data/java/train/code.original_subtoken"
    ],
    "train_src_tag": [
        "train/code.original_subtoken"
    ],
    "train_src_tag_files": [
        null
    ],
    "train_tgt": [
        "train/javadoc.original"
    ],
    "train_tgt_files": [
        "../../data/java/train/javadoc.original"
    ],
    "trans_drop": 0.2,
    "uncase": true,
    "use_all_enc_layers": false,
    "use_code_type": false,
    "use_neg_dist": true,
    "use_src_char": false,
    "use_src_word": true,
    "use_tgt_char": false,
    "use_tgt_word": true,
    "valid_metric": "bleu",
    "warmup_epochs": 0,
    "warmup_steps": 2000,
    "weight_decay": 0
} ]
06/24/2020 11:49:55 AM: [ ---------------------------------------------------------------------------------------------------- ]
06/24/2020 11:49:55 AM: [ Starting training... ]
